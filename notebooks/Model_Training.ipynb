{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "923cd0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "df = pd.read_csv(\"../notebooks/refined_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90772b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balace the dataset\n",
    "from imblearn import over_sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "X,y = smote.fit_resample(df.drop(columns='diagnosis'), df['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ab180c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['diagnosis'])\n",
    "y = df['diagnosis']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3d880cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((368, 15), (93, 15))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7442c741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay, precision_score, \\\n",
    "    recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4ef3e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clf(true, predicted):\n",
    "    acc = accuracy_score(true, predicted)\n",
    "    f1 = f1_score(true, predicted)\n",
    "    precision = precision_score(true, predicted)\n",
    "    recall = recall_score(true, predicted)\n",
    "    roc_auc = roc_auc_score(true, predicted)\n",
    "    return acc, f1, precision, recall, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf030ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"KNeighbors Classifier\": KNeighborsClassifier(),\n",
    "    \"XGB Classifier\": XGBClassifier(),\n",
    "    \"CatBoosting Classifier\": CatBoostClassifier(verbose=False),\n",
    "    \"SupportVector Classifier\": SVC(),\n",
    "    \"Ada Boost Classifier\": AdaBoostClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f04e7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e33322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that will evaluate the models and return a report\n",
    "\n",
    "def evaluate_models(X, y, models):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    models_list = []\n",
    "    accuracy_list = []\n",
    "    auc = []\n",
    "    print(range(len(list(models))))\n",
    "    for i in range(len(list(models))):\n",
    "        print(i)\n",
    "        model = list(models.values())[i]\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "        # training set performance\n",
    "        model_train_accuracy, model_train_f1, model_train_precision, model_train_recall, \\\n",
    "            model_train_rocauc_score = evaluate_clf(y_train, y_train_pred)\n",
    "        \n",
    "        # test set performance\n",
    "        model_test_accuracy, model_test_f1, model_test_precision, model_test_recall, \\\n",
    "            model_test_rocauc_score = evaluate_clf(y_test, y_test_pred)\n",
    "        \n",
    "        print(list(models.keys())[i])\n",
    "        models_list.append(list(models.keys())[i])\n",
    "        print('Model performance for Training set')\n",
    "        print(\"- Accuracy: {:.4f}\".format(model_train_accuracy))\n",
    "        print('- F1 score: {:.4f}'.format(model_train_f1)) \n",
    "        print('- Precision: {:.4f}'.format(model_train_precision))\n",
    "        print('- Recall: {:.4f}'.format(model_train_recall))\n",
    "        print('- Roc Auc Score: {:.4f}'.format(model_train_rocauc_score))\n",
    "\n",
    "        print('----------------------------------')\n",
    "\n",
    "        print('Model performance for Test set')\n",
    "        print('- Accuracy: {:.4f}'.format(model_test_accuracy))\n",
    "        accuracy_list.append(model_test_accuracy)\n",
    "        print('- F1 score: {:.4f}'.format(model_test_f1))\n",
    "        print('- Precision: {:.4f}'.format(model_test_precision))\n",
    "        print('- Recall: {:.4f}'.format(model_test_recall))\n",
    "        print('- Roc Auc Score: {:.4f}'.format(model_test_rocauc_score))\n",
    "        auc.append(model_test_rocauc_score)\n",
    "        print('='*35)\n",
    "        print('\\n')\n",
    "\n",
    "    report = pd.DataFrame(list(zip(models_list, accuracy_list)), columns=['Model name', 'Accuracy']).sort_values(by='Accuracy', ascending=True)\n",
    "        \n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6261f4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 9)\n",
      "0\n",
      "Random Forest\n",
      "Model performance for Training set\n",
      "- Accuracy: 1.0000\n",
      "- F1 score: 1.0000\n",
      "- Precision: 1.0000\n",
      "- Recall: 1.0000\n",
      "- Roc Auc Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9140\n",
      "- F1 score: 0.8462\n",
      "- Precision: 0.9167\n",
      "- Recall: 0.7857\n",
      "- Roc Auc Score: 0.8775\n",
      "===================================\n",
      "\n",
      "\n",
      "1\n",
      "Decision Tree\n",
      "Model performance for Training set\n",
      "- Accuracy: 1.0000\n",
      "- F1 score: 1.0000\n",
      "- Precision: 1.0000\n",
      "- Recall: 1.0000\n",
      "- Roc Auc Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.8602\n",
      "- F1 score: 0.7636\n",
      "- Precision: 0.7778\n",
      "- Recall: 0.7500\n",
      "- Roc Auc Score: 0.8288\n",
      "===================================\n",
      "\n",
      "\n",
      "2\n",
      "Gradient Boosting\n",
      "Model performance for Training set\n",
      "- Accuracy: 1.0000\n",
      "- F1 score: 1.0000\n",
      "- Precision: 1.0000\n",
      "- Recall: 1.0000\n",
      "- Roc Auc Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9355\n",
      "- F1 score: 0.8889\n",
      "- Precision: 0.9231\n",
      "- Recall: 0.8571\n",
      "- Roc Auc Score: 0.9132\n",
      "===================================\n",
      "\n",
      "\n",
      "3\n",
      "Logistic Regression\n",
      "Model performance for Training set\n",
      "- Accuracy: 0.9837\n",
      "- F1 score: 0.9706\n",
      "- Precision: 0.9900\n",
      "- Recall: 0.9519\n",
      "- Roc Auc Score: 0.9741\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9677\n",
      "- F1 score: 0.9434\n",
      "- Precision: 1.0000\n",
      "- Recall: 0.8929\n",
      "- Roc Auc Score: 0.9464\n",
      "===================================\n",
      "\n",
      "\n",
      "4\n",
      "KNeighbors Classifier\n",
      "Model performance for Training set\n",
      "- Accuracy: 0.9674\n",
      "- F1 score: 0.9406\n",
      "- Precision: 0.9694\n",
      "- Recall: 0.9135\n",
      "- Roc Auc Score: 0.9510\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9247\n",
      "- F1 score: 0.8627\n",
      "- Precision: 0.9565\n",
      "- Recall: 0.7857\n",
      "- Roc Auc Score: 0.8852\n",
      "===================================\n",
      "\n",
      "\n",
      "5\n",
      "XGB Classifier\n",
      "Model performance for Training set\n",
      "- Accuracy: 1.0000\n",
      "- F1 score: 1.0000\n",
      "- Precision: 1.0000\n",
      "- Recall: 1.0000\n",
      "- Roc Auc Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9462\n",
      "- F1 score: 0.9091\n",
      "- Precision: 0.9259\n",
      "- Recall: 0.8929\n",
      "- Roc Auc Score: 0.9310\n",
      "===================================\n",
      "\n",
      "\n",
      "6\n",
      "CatBoosting Classifier\n",
      "Model performance for Training set\n",
      "- Accuracy: 1.0000\n",
      "- F1 score: 1.0000\n",
      "- Precision: 1.0000\n",
      "- Recall: 1.0000\n",
      "- Roc Auc Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9462\n",
      "- F1 score: 0.9057\n",
      "- Precision: 0.9600\n",
      "- Recall: 0.8571\n",
      "- Roc Auc Score: 0.9209\n",
      "===================================\n",
      "\n",
      "\n",
      "7\n",
      "SupportVector Classifier\n",
      "Model performance for Training set\n",
      "- Accuracy: 0.9891\n",
      "- F1 score: 0.9804\n",
      "- Precision: 1.0000\n",
      "- Recall: 0.9615\n",
      "- Roc Auc Score: 0.9808\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9462\n",
      "- F1 score: 0.9057\n",
      "- Precision: 0.9600\n",
      "- Recall: 0.8571\n",
      "- Roc Auc Score: 0.9209\n",
      "===================================\n",
      "\n",
      "\n",
      "8\n",
      "Ada Boost Classifier\n",
      "Model performance for Training set\n",
      "- Accuracy: 1.0000\n",
      "- F1 score: 1.0000\n",
      "- Precision: 1.0000\n",
      "- Recall: 1.0000\n",
      "- Roc Auc Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9570\n",
      "- F1 score: 0.9259\n",
      "- Precision: 0.9615\n",
      "- Recall: 0.8929\n",
      "- Roc Auc Score: 0.9387\n",
      "===================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_model_report = evaluate_models(X = X, y= y, models=models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13acfe44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.870968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.903226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighbors Classifier</td>\n",
       "      <td>0.924731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.935484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGB Classifier</td>\n",
       "      <td>0.946237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CatBoosting Classifier</td>\n",
       "      <td>0.946237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SupportVector Classifier</td>\n",
       "      <td>0.946237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.956989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.967742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model name  Accuracy\n",
       "1             Decision Tree  0.870968\n",
       "0             Random Forest  0.903226\n",
       "4     KNeighbors Classifier  0.924731\n",
       "2         Gradient Boosting  0.935484\n",
       "5            XGB Classifier  0.946237\n",
       "6    CatBoosting Classifier  0.946237\n",
       "7  SupportVector Classifier  0.946237\n",
       "8      Ada Boost Classifier  0.956989\n",
       "3       Logistic Regression  0.967742"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b942c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter tuning\n",
    "\n",
    "logistic_regression_params = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': [100, 200, 300, 500]\n",
    "}\n",
    "\n",
    "adaboost_params = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "svc_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [2, 3, 4]\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.5, 0.7, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.7, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55731216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models for hyper parameter tuning\n",
    "randomcv_models = [\n",
    "    ('XGBClassifier', XGBClassifier(), xgb_params),\n",
    "    ('SVClassifier', SVC(), svc_params),\n",
    "    ('AdaBoostClassifier', AdaBoostClassifier(), adaboost_params),\n",
    "    ('LogisticRegression', LogisticRegression(), logistic_regression_params)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8efc04af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=10, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=10, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=10, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=10, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=10, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=10, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=10, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=10, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=10, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=10, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=10, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=10, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=10, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=10, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=10, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=10, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=10, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=10, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=10, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=10, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=10, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=10, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=10, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=10, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=10, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=7, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=5, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=5, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=10, n_estimators=200, subsample=0.5; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, n_estimators=50, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=10, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.3, max_depth=3, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=5, n_estimators=100, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=10, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.7, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.7; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=5, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=50, subsample=0.5; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=50, subsample=0.5; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:318: UserWarning: The total space of parameters 96 is smaller than n_iter=100. Running 96 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n",
      "[CV] END ........C=0.1, degree=2, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=2, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=2, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=0.1, degree=2, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........C=0.1, degree=2, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........C=0.1, degree=2, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=2, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=2, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=2, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......C=0.1, degree=2, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......C=0.1, degree=2, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......C=0.1, degree=2, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=0.1, degree=2, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=0.1, degree=2, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=0.1, degree=2, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=2, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=2, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=2, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=0.1, degree=2, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=0.1, degree=2, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=0.1, degree=2, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=2, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=2, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=2, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=3, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=3, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=3, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=0.1, degree=3, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........C=0.1, degree=3, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........C=0.1, degree=3, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=3, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=3, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=3, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......C=0.1, degree=3, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......C=0.1, degree=3, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......C=0.1, degree=3, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=0.1, degree=3, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=0.1, degree=3, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=0.1, degree=3, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=3, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=3, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=3, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=0.1, degree=3, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=0.1, degree=3, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=0.1, degree=3, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=3, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=3, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=3, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=4, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=4, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=4, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=0.1, degree=4, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........C=0.1, degree=4, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........C=0.1, degree=4, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=4, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=4, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=4, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......C=0.1, degree=4, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......C=0.1, degree=4, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......C=0.1, degree=4, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=0.1, degree=4, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=0.1, degree=4, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=0.1, degree=4, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=4, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=4, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=0.1, degree=4, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=0.1, degree=4, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=0.1, degree=4, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=0.1, degree=4, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=4, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=4, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=0.1, degree=4, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=2, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=2, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=2, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ............C=1, degree=2, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=1, degree=2, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=1, degree=2, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END .............C=1, degree=2, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .............C=1, degree=2, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .............C=1, degree=2, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........C=1, degree=2, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=1, degree=2, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=1, degree=2, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...........C=1, degree=2, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=1, degree=2, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=1, degree=2, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .............C=1, degree=2, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END .............C=1, degree=2, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END .............C=1, degree=2, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ..............C=1, degree=2, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..............C=1, degree=2, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..............C=1, degree=2, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=2, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=2, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=2, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=3, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=3, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=3, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ............C=1, degree=3, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=1, degree=3, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=1, degree=3, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END .............C=1, degree=3, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .............C=1, degree=3, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .............C=1, degree=3, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........C=1, degree=3, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=1, degree=3, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=1, degree=3, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...........C=1, degree=3, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=1, degree=3, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=1, degree=3, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .............C=1, degree=3, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END .............C=1, degree=3, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END .............C=1, degree=3, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ..............C=1, degree=3, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..............C=1, degree=3, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..............C=1, degree=3, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=3, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=3, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=3, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=4, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=4, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=4, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ............C=1, degree=4, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=1, degree=4, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=1, degree=4, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END .............C=1, degree=4, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .............C=1, degree=4, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .............C=1, degree=4, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........C=1, degree=4, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=1, degree=4, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=1, degree=4, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...........C=1, degree=4, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=1, degree=4, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=1, degree=4, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .............C=1, degree=4, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END .............C=1, degree=4, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END .............C=1, degree=4, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ..............C=1, degree=4, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..............C=1, degree=4, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..............C=1, degree=4, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=4, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=4, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..........C=1, degree=4, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=10, degree=2, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=10, degree=2, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=10, degree=2, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=10, degree=2, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=10, degree=2, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=10, degree=2, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=10, degree=2, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=10, degree=2, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=10, degree=2, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........C=10, degree=2, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=10, degree=2, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=10, degree=2, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..........C=10, degree=2, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=10, degree=2, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=10, degree=2, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ............C=10, degree=2, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=10, degree=2, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=10, degree=2, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END .............C=10, degree=2, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END .............C=10, degree=2, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END .............C=10, degree=2, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........C=10, degree=2, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=10, degree=2, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=10, degree=2, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=10, degree=3, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=10, degree=3, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=10, degree=3, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=10, degree=3, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=10, degree=3, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=10, degree=3, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=10, degree=3, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=10, degree=3, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=10, degree=3, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........C=10, degree=3, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=10, degree=3, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=10, degree=3, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..........C=10, degree=3, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=10, degree=3, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=10, degree=3, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ............C=10, degree=3, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=10, degree=3, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=10, degree=3, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END .............C=10, degree=3, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END .............C=10, degree=3, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END .............C=10, degree=3, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........C=10, degree=3, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=10, degree=3, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=10, degree=3, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=10, degree=4, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=10, degree=4, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=10, degree=4, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=10, degree=4, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=10, degree=4, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=10, degree=4, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=10, degree=4, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=10, degree=4, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=10, degree=4, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........C=10, degree=4, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=10, degree=4, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=10, degree=4, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..........C=10, degree=4, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=10, degree=4, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=10, degree=4, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ............C=10, degree=4, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=10, degree=4, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=10, degree=4, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END .............C=10, degree=4, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END .............C=10, degree=4, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END .............C=10, degree=4, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........C=10, degree=4, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=10, degree=4, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=10, degree=4, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=100, degree=2, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ........C=100, degree=2, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ........C=100, degree=2, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=100, degree=2, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........C=100, degree=2, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........C=100, degree=2, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=2, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=2, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=2, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......C=100, degree=2, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......C=100, degree=2, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......C=100, degree=2, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=100, degree=2, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=100, degree=2, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=100, degree=2, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=2, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=2, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=2, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=100, degree=2, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=100, degree=2, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=100, degree=2, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........C=100, degree=2, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=100, degree=2, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=100, degree=2, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=100, degree=3, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ........C=100, degree=3, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ........C=100, degree=3, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=100, degree=3, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........C=100, degree=3, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........C=100, degree=3, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=3, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=3, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=3, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......C=100, degree=3, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......C=100, degree=3, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......C=100, degree=3, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=100, degree=3, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=100, degree=3, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=100, degree=3, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=3, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=3, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=3, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=100, degree=3, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=100, degree=3, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=100, degree=3, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........C=100, degree=3, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=100, degree=3, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=100, degree=3, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=100, degree=4, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ........C=100, degree=4, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ........C=100, degree=4, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..........C=100, degree=4, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........C=100, degree=4, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........C=100, degree=4, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=4, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=4, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=4, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......C=100, degree=4, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......C=100, degree=4, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......C=100, degree=4, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .........C=100, degree=4, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=100, degree=4, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .........C=100, degree=4, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=4, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=4, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...........C=100, degree=4, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ............C=100, degree=4, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=100, degree=4, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ............C=100, degree=4, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........C=100, degree=4, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=100, degree=4, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........C=100, degree=4, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=50; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:318: UserWarning: The total space of parameters 32 is smaller than n_iter=100. Running 32 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=50; total time=   0.0s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=50; total time=   0.0s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=100; total time=   0.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=100; total time=   0.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=100; total time=   0.2s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=150; total time=   0.3s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=150; total time=   0.3s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=150; total time=   0.3s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=200; total time=   0.4s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=200; total time=   0.4s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.01, n_estimators=200; total time=   0.4s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=50; total time=   0.0s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=50; total time=   0.0s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=50; total time=   0.0s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=100; total time=   0.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=100; total time=   0.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=100; total time=   0.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=150; total time=   0.2s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=150; total time=   0.2s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=150; total time=   0.3s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=200; total time=   0.4s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=200; total time=   0.4s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.1, n_estimators=200; total time=   0.4s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.5, n_estimators=50; total time=   0.0s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.5, n_estimators=50; total time=   0.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.5, n_estimators=50; total time=   0.1s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.5, n_estimators=100; total time=   0.4s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.5, n_estimators=100; total time=   0.4s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.5, n_estimators=100; total time=   0.4s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.5, n_estimators=150; total time=   0.6s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.5, n_estimators=150; total time=   0.2s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.5, n_estimators=150; total time=   0.2s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.5, n_estimators=200; total time=   0.4s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.5, n_estimators=200; total time=   0.4s\n",
      "[CV] END algorithm=SAMME, learning_rate=0.5, n_estimators=200; total time=   0.4s\n",
      "[CV] END ..algorithm=SAMME, learning_rate=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ..algorithm=SAMME, learning_rate=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END ..algorithm=SAMME, learning_rate=1, n_estimators=50; total time=   0.0s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=100; total time=   0.1s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=100; total time=   0.2s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=100; total time=   0.2s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=150; total time=   0.3s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=150; total time=   0.3s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=150; total time=   0.3s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=200; total time=   0.5s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=200; total time=   0.4s\n",
      "[CV] END .algorithm=SAMME, learning_rate=1, n_estimators=200; total time=   0.4s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50; total time=   0.0s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=50; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=100; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=150; total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=150; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=150; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=200; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=200; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.01, n_estimators=200; total time=   0.4s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50; total time=   0.0s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=50; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=100; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=150; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=150; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=150; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=200; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=200; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.1, n_estimators=200; total time=   0.4s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.5, n_estimators=50; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.5, n_estimators=50; total time=   0.0s\n",
      "[CV] END algorithm=SAMME.R, learning_rate=0.5, n_estimators=50; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.5, n_estimators=100; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.5, n_estimators=100; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.5, n_estimators=100; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.5, n_estimators=150; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.5, n_estimators=150; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.5, n_estimators=150; total time=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.5, n_estimators=200; total time=   1.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.5, n_estimators=200; total time=   1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=0.5, n_estimators=200; total time=   1.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=50; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=50; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=50; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=100; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=100; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=100; total time=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=150; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=150; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=150; total time=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=200; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=200; total time=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END algorithm=SAMME.R, learning_rate=1, n_estimators=200; total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "[CV] END ......C=0.1, max_iter=300, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=300, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=300, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...C=10, max_iter=100, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=10, max_iter=100, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=10, max_iter=100, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ........C=100, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=100, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=100, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END ....C=1, max_iter=100, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ....C=1, max_iter=100, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ....C=1, max_iter=100, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=10, max_iter=500, penalty=elasticnet, solver=lbfgs; total time=   0.0s\n",
      "[CV] END C=10, max_iter=500, penalty=elasticnet, solver=lbfgs; total time=   0.0s\n",
      "[CV] END C=10, max_iter=500, penalty=elasticnet, solver=lbfgs; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=300, penalty=elasticnet, solver=sag; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=300, penalty=elasticnet, solver=sag; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=300, penalty=elasticnet, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.1, max_iter=100, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END .......C=0.1, max_iter=100, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END .......C=0.1, max_iter=100, penalty=l1, solver=saga; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .........C=10, max_iter=300, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=10, max_iter=300, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=10, max_iter=300, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END ..........C=1, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END ..........C=1, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END ..........C=1, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=100, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=100, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=100, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=500, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=500, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=500, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=100, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=100, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=100, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.01, max_iter=500, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.01, max_iter=500, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.01, max_iter=500, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END C=10, max_iter=300, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=10, max_iter=300, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=10, max_iter=300, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ......C=10, max_iter=300, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=10, max_iter=300, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=10, max_iter=300, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=100, max_iter=200, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......C=100, max_iter=200, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......C=100, max_iter=200, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ........C=0.1, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=0.1, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=0.1, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=100, max_iter=100, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=100, max_iter=100, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=100, max_iter=100, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.01, max_iter=300, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.01, max_iter=300, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.01, max_iter=300, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=200, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=200, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=200, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..C=100, max_iter=200, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ..C=100, max_iter=200, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ..C=100, max_iter=200, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=300, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=300, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=300, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .....C=100, max_iter=500, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .....C=100, max_iter=500, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .....C=100, max_iter=500, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ..C=0.1, max_iter=300, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..C=0.1, max_iter=300, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..C=0.1, max_iter=300, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=500, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=500, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......C=0.1, max_iter=500, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END C=1, max_iter=200, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=1, max_iter=200, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=1, max_iter=200, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END .........C=1, max_iter=100, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END .........C=1, max_iter=100, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END .........C=1, max_iter=100, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END .........C=10, max_iter=500, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=10, max_iter=500, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=10, max_iter=500, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END ...C=10, max_iter=500, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ...C=10, max_iter=500, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ...C=10, max_iter=500, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=10, max_iter=500, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=10, max_iter=500, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=10, max_iter=500, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ....C=100, max_iter=500, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ....C=100, max_iter=500, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ....C=100, max_iter=500, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ....C=1, max_iter=200, penalty=l1, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ....C=1, max_iter=200, penalty=l1, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ....C=1, max_iter=200, penalty=l1, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .........C=1, max_iter=300, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END .........C=1, max_iter=300, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END .........C=1, max_iter=300, penalty=l1, solver=saga; total time=   0.0s\n",
      "[CV] END .....C=100, max_iter=200, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .....C=100, max_iter=200, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .....C=100, max_iter=200, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .C=1, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .C=1, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .C=1, max_iter=300, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .......C=0.01, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.01, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.01, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=300, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=300, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=300, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...C=10, max_iter=100, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=10, max_iter=100, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=10, max_iter=100, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=10, max_iter=500, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=10, max_iter=500, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=10, max_iter=500, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=200, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=200, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=200, penalty=none, solver=liblinear; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ........C=100, max_iter=500, penalty=l2, solver=sag; total time=   0.1s\n",
      "[CV] END ........C=100, max_iter=500, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=100, max_iter=500, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=500, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=500, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=500, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=300, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=300, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=300, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .........C=10, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=10, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=10, max_iter=200, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END C=100, max_iter=300, penalty=elasticnet, solver=lbfgs; total time=   0.0s\n",
      "[CV] END C=100, max_iter=300, penalty=elasticnet, solver=lbfgs; total time=   0.0s\n",
      "[CV] END C=100, max_iter=300, penalty=elasticnet, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..C=100, max_iter=300, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..C=100, max_iter=300, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..C=100, max_iter=300, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ........C=0.1, max_iter=200, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=0.1, max_iter=200, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=0.1, max_iter=200, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END .C=10, max_iter=300, penalty=elasticnet, solver=sag; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .C=10, max_iter=300, penalty=elasticnet, solver=sag; total time=   0.0s\n",
      "[CV] END .C=10, max_iter=300, penalty=elasticnet, solver=sag; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=300, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=300, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=300, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=300, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=300, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=300, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=1, max_iter=300, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=1, max_iter=300, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=1, max_iter=300, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=100, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=100, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=100, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ...C=0.01, max_iter=300, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...C=0.01, max_iter=300, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ...C=0.01, max_iter=300, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ....C=1, max_iter=100, penalty=l1, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ....C=1, max_iter=100, penalty=l1, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ....C=1, max_iter=100, penalty=l1, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=10, max_iter=200, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=10, max_iter=200, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=10, max_iter=200, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=1, max_iter=100, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=1, max_iter=100, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=1, max_iter=100, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=1, max_iter=500, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .C=1, max_iter=500, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .C=1, max_iter=500, penalty=elasticnet, solver=saga; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=500, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=500, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=500, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .......C=10, max_iter=300, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .......C=10, max_iter=300, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .......C=10, max_iter=300, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=300, penalty=l1, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=300, penalty=l1, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=300, penalty=l1, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .......C=0.01, max_iter=300, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.01, max_iter=300, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END .......C=0.01, max_iter=300, penalty=l2, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=200, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=200, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=200, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......C=10, max_iter=200, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=10, max_iter=200, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=10, max_iter=200, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=200, penalty=l1, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=200, penalty=l1, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=200, penalty=l1, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=10, max_iter=300, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=10, max_iter=300, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=10, max_iter=300, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=100, max_iter=200, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=100, max_iter=200, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=100, max_iter=200, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=200, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=200, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .......C=1, max_iter=200, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END C=10, max_iter=500, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=10, max_iter=500, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=10, max_iter=500, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END ....C=1, max_iter=100, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ....C=1, max_iter=100, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END ....C=1, max_iter=100, penalty=l1, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=500, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=500, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=500, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .....C=0.01, max_iter=100, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .....C=0.01, max_iter=100, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .....C=0.01, max_iter=100, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=100, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=100, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=100, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ......C=0.01, max_iter=100, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=0.01, max_iter=100, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=0.01, max_iter=100, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ...C=10, max_iter=200, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ...C=10, max_iter=200, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ...C=10, max_iter=200, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=100, penalty=elasticnet, solver=lbfgs; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=100, penalty=elasticnet, solver=lbfgs; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=100, penalty=elasticnet, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=0.01, max_iter=500, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=0.01, max_iter=500, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=0.01, max_iter=500, penalty=l2, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......C=1, max_iter=100, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......C=1, max_iter=100, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......C=1, max_iter=100, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .......C=10, max_iter=500, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .......C=10, max_iter=500, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .......C=10, max_iter=500, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END C=100, max_iter=500, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=100, max_iter=500, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=100, max_iter=500, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END .........C=1, max_iter=500, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END .........C=1, max_iter=500, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END .........C=1, max_iter=500, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=500, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=500, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=500, penalty=none, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .....C=10, max_iter=100, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=10, max_iter=100, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=10, max_iter=100, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=200, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=200, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END .C=0.01, max_iter=200, penalty=l2, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=200, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=200, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END C=0.01, max_iter=200, penalty=elasticnet, solver=newton-cg; total time=   0.0s\n",
      "[CV] END ....C=100, max_iter=200, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ....C=100, max_iter=200, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ....C=100, max_iter=200, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......C=1, max_iter=500, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......C=1, max_iter=500, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ......C=1, max_iter=500, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ..C=1, max_iter=500, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..C=1, max_iter=500, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..C=1, max_iter=500, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=200, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=200, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END ........C=1, max_iter=200, penalty=none, solver=sag; total time=   0.0s\n",
      "[CV] END ......C=10, max_iter=100, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=10, max_iter=100, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ......C=10, max_iter=100, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END .......C=0.1, max_iter=200, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END .......C=0.1, max_iter=200, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END .......C=0.1, max_iter=200, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ..C=1, max_iter=200, penalty=elasticnet, solver=sag; total time=   0.0s\n",
      "[CV] END ..C=1, max_iter=200, penalty=elasticnet, solver=sag; total time=   0.0s\n",
      "[CV] END ..C=1, max_iter=200, penalty=elasticnet, solver=sag; total time=   0.0s\n",
      "[CV] END ....C=0.01, max_iter=300, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ....C=0.01, max_iter=300, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ....C=0.01, max_iter=300, penalty=none, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=10, max_iter=300, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END ........C=10, max_iter=300, penalty=l2, solver=saga; total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:542: FitFailedWarning: \n",
      "198 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "87 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\base.py\", line 1344, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "21 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 75, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\base.py\", line 1351, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1182, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.95448887 0.95662507 0.95878958 0.96961209        nan        nan\n",
      " 0.93717285 0.96095408 0.96743344        nan 0.71366324        nan\n",
      "        nan        nan        nan        nan 0.95448887        nan\n",
      "        nan        nan 0.95011742        nan        nan 0.93926662\n",
      " 0.95448887        nan 0.96743344        nan 0.96095408        nan\n",
      "        nan        nan 0.97179074        nan        nan 0.89596243\n",
      "        nan 0.96095408        nan        nan 0.95228192 0.9219223\n",
      "        nan 0.96095408        nan 0.94795292        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.89596243        nan        nan\n",
      " 0.89596243 0.96743344        nan        nan        nan        nan\n",
      "        nan        nan 0.96961209        nan        nan 0.89596243\n",
      " 0.89596243 0.96095408        nan 0.89596243        nan        nan\n",
      "        nan 0.96743344        nan        nan 0.89596243        nan\n",
      "        nan        nan        nan        nan        nan 0.95448887\n",
      "        nan        nan 0.96311858        nan        nan 0.95011742\n",
      "        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ........C=10, max_iter=300, penalty=l2, solver=saga; total time=   0.0s\n",
      "[CV] END C=1, max_iter=200, penalty=elasticnet, solver=lbfgs; total time=   0.0s\n",
      "[CV] END C=1, max_iter=200, penalty=elasticnet, solver=lbfgs; total time=   0.0s\n",
      "[CV] END C=1, max_iter=200, penalty=elasticnet, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .........C=10, max_iter=200, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=10, max_iter=200, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END .........C=10, max_iter=200, penalty=l1, solver=sag; total time=   0.0s\n",
      "[CV] END ..C=100, max_iter=200, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..C=100, max_iter=200, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ..C=100, max_iter=200, penalty=l2, solver=liblinear; total time=   0.0s\n",
      "[CV] END ....C=0.1, max_iter=100, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ....C=0.1, max_iter=100, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END ....C=0.1, max_iter=100, penalty=none, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=0.01, max_iter=500, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=0.01, max_iter=500, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END .....C=0.01, max_iter=500, penalty=l1, solver=lbfgs; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=300, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=300, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=0.1, max_iter=300, penalty=none, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=10, max_iter=200, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=10, max_iter=200, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "[CV] END C=10, max_iter=200, penalty=elasticnet, solver=liblinear; total time=   0.0s\n",
      "Best params for XGBClassifier\n",
      "{'subsample': 0.5, 'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.3, 'colsample_bytree': 0.7}\n",
      "Best params for SVClassifier\n",
      "{'kernel': 'rbf', 'gamma': 'auto', 'degree': 2, 'C': 10}\n",
      "Best params for AdaBoostClassifier\n",
      "{'n_estimators': 150, 'learning_rate': 1, 'algorithm': 'SAMME.R'}\n",
      "Best params for LogisticRegression\n",
      "{'solver': 'saga', 'penalty': 'l1', 'max_iter': 300, 'C': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model_param = {}\n",
    "for name, model, params in randomcv_models:\n",
    "    random = RandomizedSearchCV(estimator=model, param_distributions=params,\n",
    "                                n_iter=100,\n",
    "                                cv=3,\n",
    "                                verbose=2,\n",
    "                                n_jobs=1)\n",
    "\n",
    "    random.fit(X,y)\n",
    "    model_param[name] = random.best_params_\n",
    "\n",
    "for model_name in model_param:\n",
    "    print(f\"Best params for {model_name}\")\n",
    "    print(model_param[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b5dcf27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'XGBClassifier': {'subsample': 0.5,\n",
       "  'n_estimators': 50,\n",
       "  'max_depth': 5,\n",
       "  'learning_rate': 0.3,\n",
       "  'colsample_bytree': 0.7},\n",
       " 'SVClassifier': {'kernel': 'rbf', 'gamma': 'auto', 'degree': 2, 'C': 10},\n",
       " 'AdaBoostClassifier': {'n_estimators': 150,\n",
       "  'learning_rate': 1,\n",
       "  'algorithm': 'SAMME.R'},\n",
       " 'LogisticRegression': {'solver': 'saga',\n",
       "  'penalty': 'l1',\n",
       "  'max_iter': 300,\n",
       "  'C': 1}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc1c758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 4)\n",
      "0\n",
      "XGB Classifier\n",
      "Model performance for Training set\n",
      "- Accuracy: 0.9946\n",
      "- F1 score: 0.9903\n",
      "- Precision: 1.0000\n",
      "- Recall: 0.9808\n",
      "- Roc Auc Score: 0.9904\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9140\n",
      "- F1 score: 0.8462\n",
      "- Precision: 0.9167\n",
      "- Recall: 0.7857\n",
      "- Roc Auc Score: 0.8775\n",
      "===================================\n",
      "\n",
      "\n",
      "1\n",
      "SVClassifier\n",
      "Model performance for Training set\n",
      "- Accuracy: 0.9918\n",
      "- F1 score: 0.9854\n",
      "- Precision: 1.0000\n",
      "- Recall: 0.9712\n",
      "- Roc Auc Score: 0.9856\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9570\n",
      "- F1 score: 0.9259\n",
      "- Precision: 0.9615\n",
      "- Recall: 0.8929\n",
      "- Roc Auc Score: 0.9387\n",
      "===================================\n",
      "\n",
      "\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\diagno\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Boost Classifier\n",
      "Model performance for Training set\n",
      "- Accuracy: 1.0000\n",
      "- F1 score: 1.0000\n",
      "- Precision: 1.0000\n",
      "- Recall: 1.0000\n",
      "- Roc Auc Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9570\n",
      "- F1 score: 0.9259\n",
      "- Precision: 0.9615\n",
      "- Recall: 0.8929\n",
      "- Roc Auc Score: 0.9387\n",
      "===================================\n",
      "\n",
      "\n",
      "3\n",
      "Logistic Regression\n",
      "Model performance for Training set\n",
      "- Accuracy: 0.9864\n",
      "- F1 score: 0.9754\n",
      "- Precision: 1.0000\n",
      "- Recall: 0.9519\n",
      "- Roc Auc Score: 0.9760\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9785\n",
      "- F1 score: 0.9630\n",
      "- Precision: 1.0000\n",
      "- Recall: 0.9286\n",
      "- Roc Auc Score: 0.9643\n",
      "===================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retrain the model with best params\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "best_models = {\n",
    "    'XGB Classifier': XGBClassifier(**model_param['XGBClassifier'], n_jobs=1),\n",
    "    'SVClassifier': SVC(**model_param['SVClassifier']),\n",
    "    'Ada Boost Classifier': AdaBoostClassifier(**model_param['AdaBoostClassifier']),\n",
    "    'Logistic Regression': LogisticRegression(**model_param['LogisticRegression'])\n",
    "}\n",
    "\n",
    "tuned_report = evaluate_models(X = X, y = y, models = best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53a7a0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model name</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB Classifier</td>\n",
       "      <td>0.913978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVClassifier</td>\n",
       "      <td>0.956989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ada Boost Classifier</td>\n",
       "      <td>0.956989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.978495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model name  Accuracy\n",
       "0        XGB Classifier  0.913978\n",
       "1          SVClassifier  0.956989\n",
       "2  Ada Boost Classifier  0.956989\n",
       "3   Logistic Regression  0.978495"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02bea8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL MODEL 'KNN'\n",
      "Accuracy Score value: 0.9785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        65\n",
      "           1       1.00      0.93      0.96        28\n",
      "\n",
      "    accuracy                           0.98        93\n",
      "   macro avg       0.99      0.96      0.97        93\n",
      "weighted avg       0.98      0.98      0.98        93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = LogisticRegression(**model_param['LogisticRegression'])\n",
    "best_model = best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"FINAL MODEL 'KNN'\")\n",
    "print (\"Accuracy Score value: {:.4f}\".format(score))\n",
    "print (cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "794a6c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1e806dea550>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diagno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
